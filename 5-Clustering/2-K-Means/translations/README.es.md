# Agrupamiento K-Medias

[![Andrew Ng explica el agrupamiento](https://img.youtube.com/vi/hDmNF9JG3lo/0.jpg)](https://youtu.be/hDmNF9JG3lo "Andrew Ng explica el agrupamiento")

> ðŸŽ¥ Haz clic en la imagen de arriba para ver el video: Andrew Ng explica el agrupamiento"

## [Examen previo a la lecciÃ³n](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/29?loc=es)

En esta lecciÃ³n, aprenderÃ¡s cÃ³mo crear grupos usando Scikit-learn y el conjunto de datos de mÃºsica Nigeriana que importaste anteriormente. Cubriremos los conceptos bÃ¡sicos de K-Medias para agrupamiento. Ten en mente que, como aprendiste en lecciones anteriores, hay muchas formas de de trabajar con grupos y el mÃ©todo que uses depende de tus datos. Probaremos K-medias ya que es la tÃ©cnica de agrupamiento mÃ¡s comÃºn. Â¡Comencemos!

TÃ©rminos que sobre los que aprenderÃ¡s:

- Puntaje de silueta
- MÃ©todo del codo
- Inercia
- Varianza

## IntroducciÃ³n

[El agrupamiento K-medias](https://wikipedia.org/wiki/K-means_clustering) es un mÃ©todo derivado del dominio del procesamiento de seÃ±ales. Se usa para dividir y particionar grupos de datos en 'k' grupos usando una serie de observaciones. Cada observaciÃ³n funciona para agrupar un punto de datos mÃ¡s cercano a su 'media' mÃ¡s cercana, o el punto central de un grupo.

Los grupos pueden ser visualizados como [diagramas Voronoi](https://wikipedia.org/wiki/Voronoi_diagram), los cuales incluye un punto (o 'semilla') y su regiÃ³n correspondiente.

![diagrama Voronoi](../images/voronoi.png)

> InfografÃ­a de [Jen Looper](https://twitter.com/jenlooper)

El proceso de agrupamiento K-medias [se ejecuta en un proceso de tres pasos](https://scikit-learn.org/stable/modules/clustering.html#k-means):

1. El algoritmo selecciona el k-nÃºmero de puntos centrales al hacer muestreo del conjunto de datos. DespuÃ©s de esto, se repite:
    1. Se asigna cada muestra al centroide mÃ¡s cercano.
    2. Se crean nuevos centroides al tomar el valor medio de todas las muestras asignadas a los centroides previos.
    3. Luego, se calcula la diferencia entre los centroides nuevos y viejos y se repite hasta que los centroides se estabilizan.

Un inconveniente de usar K-medias incluye el hecho que necesitarÃ¡s establecer 'k', que es el nÃºmero de centroides. Afortunadamente el 'mÃ©todo del codo' ayuda a estimar un buen valor inicial para 'k'. Lo probarÃ¡s en un minuto.

## Prerrequisitos

TrabajarÃ¡s en el archivo _notebook.ipynb_ de esta lecciÃ³n, que incluye la importaciÃ³n de datos y limpieza preliminar que hiciste en la Ãºltima lecciÃ³n.

## Ejercicio - preparaciÃ³n

Comienza por darle otro vistazo a los datos de canciones.

1. Crea un grÃ¡fico de caja, llamando a `boxplot()` para cada columna:

    ```python
    plt.figure(figsize=(20,20), dpi=200)
    
    plt.subplot(4,3,1)
    sns.boxplot(x = 'popularity', data = df)
    
    plt.subplot(4,3,2)
    sns.boxplot(x = 'acousticness', data = df)
    
    plt.subplot(4,3,3)
    sns.boxplot(x = 'energy', data = df)
    
    plt.subplot(4,3,4)
    sns.boxplot(x = 'instrumentalness', data = df)
    
    plt.subplot(4,3,5)
    sns.boxplot(x = 'liveness', data = df)
    
    plt.subplot(4,3,6)
    sns.boxplot(x = 'loudness', data = df)
    
    plt.subplot(4,3,7)
    sns.boxplot(x = 'speechiness', data = df)
    
    plt.subplot(4,3,8)
    sns.boxplot(x = 'tempo', data = df)
    
    plt.subplot(4,3,9)
    sns.boxplot(x = 'time_signature', data = df)
    
    plt.subplot(4,3,10)
    sns.boxplot(x = 'danceability', data = df)
    
    plt.subplot(4,3,11)
    sns.boxplot(x = 'length', data = df)
    
    plt.subplot(4,3,12)
    sns.boxplot(x = 'release_date', data = df)
    ```

    Estos datos son un poco ruidosos: al observar cada columna como un grÃ¡fico de caja, puedes ver los valores atÃ­picos.

    ![Valores atÃ­picos](../images/boxplots.png)

PodrÃ­as revisar el conjunto de datos y remover estos valores atÃ­picos, pero eso harÃ­a que quedara un mÃ­nimo de datos.

1. Por ahora, elege quÃ© columnas usarÃ¡s para tu ejercicio de agrupamiento. Elige unas con rangos similares y codifica la columna `artist_top_genre` como datos numÃ©ricos:

    ```python
    from sklearn.preprocessing import LabelEncoder
    le = LabelEncoder()
    
    X = df.loc[:, ('artist_top_genre','popularity','danceability','acousticness','loudness','energy')]
    
    y = df['artist_top_genre']
    
    X['artist_top_genre'] = le.fit_transform(X['artist_top_genre'])
    
    y = le.transform(y)
    ```

1. Ahora necesitas elegir a cuÃ¡ntos grupos apuntar. Sabes que hay 3 gÃ©neros de canciones que extrajimos de el conjunto de datos, asÃ­ que probemos con 3:

    ```python
    from sklearn.cluster import KMeans
    
    nclusters = 3 
    seed = 0
    
    km = KMeans(n_clusters=nclusters, random_state=seed)
    km.fit(X)
    
    # Predict the cluster for each data point
    
    y_cluster_kmeans = km.predict(X)
    y_cluster_kmeans
    ```

Ves un arreglo impreso con los grupos predichos (0, 1, 0 2) para cada fila del dataframe.

1. Usa este arreglo para calcular una 'puntaje de silueta':

    ```python
    from sklearn import metrics
    score = metrics.silhouette_score(X, y_cluster_kmeans)
    score
    ```

## Puntaje de silueta

Busca un puntaje de silueta mÃ¡s cercano a 1. Este puntaje varÃ­a de -1 a 1, y si el puntaje es 1, el grupo es denso y bien separado de otros grupos. Un valor cercano a 0 representa grupos superpuestos con muestras muy cercanas al lÃ­mite de decisiÃ³n de los grupos vecinos. [Fuente](https://dzone.com/articles/kmeans-silhouette-score-explained-with-python-exam).

Nuestro puntaje es de **.53**, justo a la mitad. Esto indica que nuestros datos no son particularmente adecuados para este tipo de agrupamiento, pero continuemos.

### Ejercicio - construye un modelo

1. Importa `KMeans` e inicia el proceso de agrupamiento.

    ```python
    from sklearn.cluster import KMeans
    wcss = []
    
    for i in range(1, 11):
        kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)
        kmeans.fit(X)
        wcss.append(kmeans.inertia_)
    
    ```

    Hay algunas partes que requieren explicaciÃ³n.

    > ðŸŽ“ range: Estas son las iteraciones del proceso de agrupamiento

    > ðŸŽ“ random_state: "Determina la generaciÃ³n de nÃºmeros aleatorios para la inicializaciÃ³n del centroide." [Fuente](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans)

    > ðŸŽ“ WCSS: "within-cluster sums of squares (suma de cuadrados dentro del grupo)" mide la distancia cuadrÃ¡tica promedio de todos los puntos dentro de un grupo al centroide dle grupo. [Fuente](https://medium.com/@ODSC/unsupervised-learning-evaluating-clusters-bd47eed175ce).

    > ðŸŽ“ Inertia: Los algoritmos K-medias intentan elegir los centroides para minimizar la 'inertia (inercia)', "una medida de cuÃ¡nta coherencia interna  tienen los grupos." [Fuente](https://scikit-learn.org/stable/modules/clustering.html). El valor se agrega a la variable wcss en cada iteraciÃ³n.

    > ðŸŽ“ k-means++: En [Scikit-learn](https://scikit-learn.org/stable/modules/clustering.html#k-means) puedes usar la optimizaciÃ³n 'k-means++', la cual "inicializa los centroides para que sean (generalmente) distantes uno de otro, llevando a probablemente mejores resultados que la inicializaciÃ³n aleatoria".

### MÃ©todo del codo

Anteriormente, supusiste que, porque has apuntado a 3 gÃ©neros de canciones, deberÃ­as elegir 3 grupos. Â¿Pero es el caso?

1. Usa el 'mÃ©todo del codo' para asegurarte.

    ```python
    plt.figure(figsize=(10,5))
    sns.lineplot(range(1, 11), wcss,marker='o',color='red')
    plt.title('Elbow')
    plt.xlabel('Number of clusters')
    plt.ylabel('WCSS')
    plt.show()
    ```

    Usa la variable `wcss` que construiste en el paso anterior para crear una grÃ¡fica que muestre dÃ³nde se estÃ¡ 'la curva' en el codo, la cual indica el nÃºmero Ã³ptimo de grupos. Â¡QuizÃ¡ **es** 3!

    ![MÃ©todo del codo](../images/elbow.png)

## Ejercicio - muestra los grupos

1. Prueba el proceso de nuevo, esta vez configurando 3 grupos, y muestra los grupos como un grÃ¡fico de dispersiÃ³n:

    ```python
    from sklearn.cluster import KMeans
    kmeans = KMeans(n_clusters = 3)
    kmeans.fit(X)
    labels = kmeans.predict(X)
    plt.scatter(df['popularity'],df['danceability'],c = labels)
    plt.xlabel('popularity')
    plt.ylabel('danceability')
    plt.show()
    ```

1. Revisa la precisiÃ³n del modelo:

    ```python
    labels = kmeans.labels_
    
    correct_labels = sum(y == labels)
    
    print("Result: %d out of %d samples were correctly labeled." % (correct_labels, y.size))
    
    print('Accuracy score: {0:0.2f}'. format(correct_labels/float(y.size)))
    ```

    La precisiÃ³n de este modelo no es tan buena, y la forma de los grupos te darÃ¡ una pista del por quÃ©.

    ![Grupos](../images/clusters.png)

    Estos datos estÃ¡n demasiado desequilibrados, muy poco correlacionados y tienen demasiada varianza entre los valores de columna para agrupar bien. De hecho, los grupos que forman estÃ¡n probablemente fuertemente influenciados o sesgados por las tres categorÃ­as de gÃ©neros que definimos arriba. Â¡Eso fue un proceso de aprendizaje!

    En la documentaciÃ³n de Scikit-learn, puedes ver que un modelo como este, con grupos no muy bien demarcados, tienen un problema de 'varianza':

    ![Modelos de problemas](../images/problems.png)
    > InfografÃ­a de Scikit-learn

## Varianza

La varianza se define como "ep promedio de diferencias cuadrÃ¡ticas de la media". [Fuente](https://www.mathsisfun.com/data/standard-deviation.html). En el contexto de este problema de agrupamiento, se refiere a los datos en los que los nÃºmeros de nuestro conjunto de datos tienden a divergir demasiado de la media.

âœ… Este es un buen momento para pensar acerca de todas las formas en que podrÃ­as corregir este problema. Â¿Modificar los datos un poco mÃ¡s? 'Usar columnas distintas? Â¿Usar un algoritmo diferente? Intenta [escalando tus datos](https://www.mygreatlearning.com/blog/learning-data-science-with-k-means-clustering/) para normalizarlos y probar otras columnas.

> Prueba esta '[calculadora de varianza](https://www.calculatorsoup.com/calculators/statistics/variance-calculator.php)' para entender un poca mÃ¡s el concepto.

---

## ðŸš€DesafÃ­o

Dedica algo de tiempo a este notebook, ajustando los parÃ¡metros. Â¿Puedes mejorar la precisiÃ³n del modelo al limpiar mÃ¡s los datos (eliminando valores atÃ­picos, por ejemplo)? Puedes usar pesos para dar mayor ponderaciÃ³n a las muestras de datos proporcionadas. Â¿QuÃ© mÃ¡s puedes hacer para crear mejores grupos?

Pista: Prueba escalar tus datos. Hay cÃ³digo comentado en el notebook que agrega escalado estÃ¡ndar para hacer que las columnas de datos se parezcan mÃ¡s entre sÃ­ en tÃ©rminos de rango. EncontrarÃ¡s que mientras el puntaje de silueta disminuye el 'pliegue' en la grÃ¡fica de codo se suaviza. Esto es por quÃ© al dejar los datos sin escalar le permite a los datos con menos variaciÃ³n tengan mÃ¡s peso. Lee un poco mÃ¡s de este problema [aquÃ­](https://stats.stackexchange.com/questions/21222/are-mean-normalization-and-feature-scaling-needed-for-k-means-clustering/21226#21226).

## [Examen posterior a la lecciÃ³n](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/30?loc=es)

## RevisiÃ³n y auto-estudio

Da un vistazo a un simulador K-Medias [como este](https://user.ceng.metu.edu.tr/~akifakkus/courses/ceng574/k-means/). Puedes usar esta herramienta para visualizar puntos de datos de muestra y determina sus centroides. Puedes editar la aleatoriedad de los datos, el nÃºmero de grupos y el nÃºmero de centroides. Â¿Esto te ayuda para tener una idea de cÃ³mo se pueden agrupar los datos?

TambiÃ©n, da un vistazo a [este folleto de K-Medias](https://stanford.edu/~cpiech/cs221/handouts/kmeans.html) de Stanford.

## AsignaciÃ³n

[Prueba distintos mÃ©todos de agrupamiento](assignment.es.md)
