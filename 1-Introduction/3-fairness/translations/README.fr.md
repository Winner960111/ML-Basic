# Equit√© dans le Machine Learning 
 
![R√©sum√© de l'√©quit√© dans le Machine Learning dans un sketchnote](../../sketchnotes/ml-fairness.png)
> Sketchnote par [Tomomi Imura](https://www.twitter.com/girlie_mac)

## [Quiz pr√©alable](https://white-water-09ec41f0f.azurestaticapps.net/quiz/5/?loc=fr)
 
## Introduction

Dans ce programme, nous allons d√©couvrir comment le Machine Learning peut avoir un impact sur notre vie quotidienne. Encore aujourd'hui, les syst√®mes et les mod√®les sont impliqu√©s quotidiennement dans les t√¢ches de prise de d√©cision, telles que les diagnostics de soins ou la d√©tection de fraudes. Il est donc important que ces mod√®les fonctionnent bien afin de fournir des r√©sultats √©quitables pour tout le monde.

Imaginons ce qui peut arriver lorsque les donn√©es que nous utilisons pour construire ces mod√®les manquent de certaines donn√©es d√©mographiques, telles que la race, le sexe, les opinions politiques, la religion ou repr√©sentent de mani√®re disproportionn√©e ces donn√©es d√©mographiques. Qu'en est-il lorsque la sortie du mod√®le est interpr√©t√©e pour favoriser certains √©l√©ments d√©mographiques¬†? Quelle est la cons√©quence pour l'application l'utilisant ?

Dans cette le√ßon, nous :

- Sensibiliserons √† l'importance de l'√©quit√© dans le Machine Learning.
- Apprenderons sur les pr√©judices li√©s √† l'√©quit√©.
- Apprenderons sur l'√©valuation et l'att√©nuation des injustices.

## Pr√©requis

En tant que pr√©requis, veuillez lire le guide des connaissances sur les "Principes de l'IA responsable" et regarder la vid√©o sur le sujet suivant :

En apprendre plus sur l'IA responsable en suivant ce [guide des connaissances](https://docs.microsoft.com/fr-fr/learn/modules/responsible-ai-principles/?WT.mc_id=academic-15963-cxa)

[![L'approche de Microsoft sur l'IA responsable](https://img.youtube.com/vi/dnC8-uUZXSc/0.jpg)](https://youtu.be/dnC8-uUZXSc "Microsoft's Approach to Responsible AI")

> üé• Cliquez sur l'image ci-dessus pour la vid√©o : Microsoft's Approach to Responsible AI

## Injustices dans les donn√©es et les algorithmes

> "Si vous torturez les donn√©es assez longtemps, elles avoueront n'importe quoi" - Ronald Coase

Cette affirmation semble extr√™me, mais il est vrai que les donn√©es peuvent √™tre manipul√©es pour √©tayer n'importe quelle conclusion. Une telle manipulation peut parfois se produire involontairement. En tant qu'√™tres humains, nous avons tous des biais, et il est souvent difficile de savoir consciemment quand nous introduisons des biais dans les donn√©es.

Garantir l'√©quit√© dans l'IA et le Machine Learning reste un d√©fi sociotechnique complexe. Cela signifie qu'il ne peut pas √™tre abord√© d'un point de vue purement social ou technique.

### Dommages li√©s √† l'√©quit√©

Qu'entendons-nous par injustice ? Le terme ¬´¬†injustice¬†¬ª englobe les impacts n√©gatifs, ou ¬´¬†dommages¬†¬ª, pour un groupe de personnes, tels que ceux d√©finis en termes de race, de sexe, d'√¢ge ou de statut de handicap.

Les principaux pr√©judices li√©s √† l'√©quit√© peuvent √™tre class√©s comme suit¬†:

- **Allocation**, si un sexe ou une ethnicit√© par exemple est favoris√© par rapport √† un autre.
- **Qualit√© de service**. Si vous entra√Ænez les donn√©es pour un sc√©nario sp√©cifique mais que la r√©alit√© est plus complexe, cela r√©sulte √† de tr√®s mauvaises performances du service.
- **St√©r√©otypes**. Associer √† un groupe donn√© des attributs pr√©-assign√©s.
- **D√©nigration**. Critiquer et √©tiqueter injustement quelque chose ou quelqu'un.
- **Sur- ou sous- repr√©sentation**. L'id√©e est qu'un certain groupe n'est pas vu dans une certaine profession, et tout service ou fonction qui continue de promouvoir cette repr√©sentation contribue, in-fine, √† nuire √† ce groupe.

Regardons quelques exemples :

### Allocation

Envisageons un syst√®me hypoth√©tique de filtrage des demandes de pr√™t : le syst√®me a tendance √† choisir les hommes blancs comme de meilleurs candidats par rapport aux autres groupes. En cons√©quence, les pr√™ts sont refus√©s √† certains demandeurs.

Un autre exemple est un outil de recrutement exp√©rimental d√©velopp√© par une grande entreprise pour s√©lectionner les candidats. L'outil discriminait syst√©matiquement un sexe en utilisant des mod√®les qui ont √©t√© form√©s pour pr√©f√©rer les mots associ√©s √† d'autres. Cela a eu pour effet de p√©naliser les candidats dont les CV contiennent des mots tels que ¬´ √©quipe f√©minine de rugby ¬ª.

‚úÖ Faites une petite recherche pour trouver un exemple r√©el de ce type d'injustice.

### Qualit√© de Service

Les chercheurs ont d√©couvert que plusieurs classificateurs commerciaux de sexe avaient des taux d'erreur plus √©lev√©s autour des images de femmes avec des teins de peau plus fonc√©s par opposition aux images d'hommes avec des teins de peau plus clairs. [R√©f√©rence](https://www.media.mit.edu/publications/gender-shades-intersectional-accuracy-disparities-in-commercial-gender-classification/) 

Un autre exemple tristement c√©l√®bre est un distributeur de savon pour les mains qui ne semble pas capable de d√©tecter les personnes ayant une couleur de peau fonc√©e. [R√©f√©rence](https://www.journaldugeek.com/2017/08/18/quand-un-distributeur-automatique-de-savon-ne-reconnait-pas-les-couleurs-de-peau-foncees/)

### St√©r√©otypes

Une vision st√©r√©otyp√©e du sexe a √©t√© trouv√©e dans la traduction automatique. Lors de la traduction de ¬´ il est infirmier et elle est m√©decin ¬ª en turc, des probl√®mes ont √©t√© rencontr√©s. Le turc est une langue sans genre et poss√®de un pronom ¬´¬†o¬†¬ª pour transmettre une troisi√®me personne du singulier. Cependant, la traduction de la phrase du turc √† l'anglais donne la phrase incorrecte et st√©r√©otyp√©e suivante : ¬´¬†elle est infirmi√®re et il est m√©decin¬†¬ª.

![Traduction en turc](images/gender-bias-translate-en-tr.png)

![Traduction en anglais de nouveau](images/gender-bias-translate-tr-en.png)

### D√©nigration

Une technologie d'√©tiquetage d'images a notoirement mal √©tiquet√© les images de personnes √† la peau fonc√©e comme des gorilles. L'√©tiquetage erron√© est nocif, non seulement parce que le syst√®me fait des erreurs mais surtout car il a sp√©cifiquement appliqu√© une √©tiquette qui a pour longtemps √©t√© d√©lib√©r√©ment d√©tourn√©e pour d√©nigrer les personnes de couleurs.

[![IA : Ne suis-je pas une femme ?](https://img.youtube.com/vi/QxuyfWoVV98/0.jpg)](https://www.youtube.com/watch?v=QxuyfWoVV98 "AI, Ain't I a Woman?")
> üé• Cliquez sur l'image ci-dessus pour la vid√©o : AI, Ain't I a Woman - une performance montrant le pr√©judice caus√© par le d√©nigrement raciste par l'IA

### Sur- ou sous- repr√©sentation
 
Les r√©sultats de recherche d'images biais√©s peuvent √™tre un bon exemple de ce pr√©judice. Lorsque nous recherchons des images de professions avec un pourcentage √©gal ou sup√©rieur d'hommes que de femmes, comme l'ing√©nierie ou PDG, nous remarquons des r√©sultats qui sont plus fortement biais√©s en faveur d'un sexe donn√©.

![Recherche Bing pour PDG](images/ceos.png)
> Cette recherche sur Bing pour ¬´¬†PDG¬†¬ª produit des r√©sultats assez inclusifs

Ces cinq principaux types de pr√©judices ne sont pas mutuellement exclusifs et un m√™me syst√®me peut pr√©senter plus d'un type de pr√©judice. De plus, chaque cas varie dans sa gravit√©. Par exemple, √©tiqueter injustement quelqu'un comme un criminel est un mal beaucoup plus grave que de mal √©tiqueter une image. Il est toutefois important de se rappeler que m√™me des pr√©judices relativement peu graves peuvent causer une ali√©nation ou une isolation de personnes et l'impact cumulatif peut √™tre extr√™mement oppressant.

‚úÖ **Discussion**: Revoyez certains des exemples et voyez s'ils montrent des pr√©judices diff√©rents.  

|                         | Allocation | Qualit√© de service | St√©r√©otypes | D√©nigration | Sur- or sous- repr√©sentation |
| ----------------------- | :--------: | :----------------: | :----------: | :---------: | :----------------------------: |
| Syst√®me de recrutement automatis√© |     x      |         x          |      x       |             |               x                |
| Traduction automatique    |            |                    |              |             |                                |
| √âtiquetage des photos          |            |                    |              |             |                                |


## D√©tecter l'injustice

Il existe de nombreuses raisons pour lesquelles un syst√®me donn√© se comporte de mani√®re injuste. Les pr√©jug√©s sociaux, par exemple, pourraient se refl√©ter dans les ensembles de donn√©es utilis√©s pour les former. Par exemple, l'injustice √† l'embauche pourrait avoir √©t√© exacerb√©e par une confiance excessive dans les donn√©es historiques. Ainsi, en utilisant les curriculum vitae soumis √† l'entreprise sur une p√©riode de 10 ans, le mod√®le a d√©termin√© que les hommes √©taient plus qualifi√©s car la majorit√© des CV provenaient d'hommes, reflet de la domination masculine pass√©e dans l'industrie de la technologie.

Des donn√©es inad√©quates sur un certain groupe de personnes peuvent √™tre la cause d'une injustice. Par exemple, les classificateurs d'images avaient un taux d'erreur plus √©lev√© pour les images de personnes √† la peau fonc√©e, car les teins de peau plus fonc√©s √©taient sous-repr√©sent√©s dans les donn√©es.

Des hypoth√®ses erron√©es faites pendant le d√©veloppement causent √©galement des injustices. Par exemple, un syst√®me d'analyse faciale destin√© √† pr√©dire qui va commettre un crime sur la base d'images de visages peut conduire √† des hypoth√®ses pr√©judiciables. Cela pourrait entra√Æner des dommages substantiels pour les personnes mal class√©es.

## Comprendre vos mod√®les et instaurer l'√©quit√©
 
Bien que de nombreux aspects de l'√©quit√© ne soient pas pris en compte dans les mesures d'√©quit√© quantitatives et qu'il ne soit pas possible de supprimer compl√®tement les biais d'un syst√®me pour garantir l'√©quit√©, nous sommes toujours responsable de d√©tecter et d'att√©nuer autant que possible les probl√®mes d'√©quit√©.

Lorsque nous travaillons avec des mod√®les de Machine Learning, il est important de comprendre vos mod√®les en garantissant leur interpr√©tabilit√© et en √©valuant et en att√©nuant les injustices.

Utilisons l'exemple de s√©lection de pr√™t afin de d√©terminer le niveau d'impact de chaque facteur sur la pr√©diction.

## M√©thodes d'√©valuation

1. **Identifier les pr√©judices (et les avantages)**. La premi√®re √©tape consiste √† identifier les pr√©judices et les avantages. R√©fl√©chissez √† la fa√ßon dont les actions et les d√©cisions peuvent affecter √† la fois les clients potentiels et l'entreprise elle-m√™me.
  
1. **Identifier les groupes concern√©s**. Une fois que vous avez compris le type de pr√©judices ou d'avantages qui peuvent survenir, identifiez les groupes susceptibles d'√™tre touch√©s. Ces groupes sont-ils d√©finis par le sexe, l'origine ethnique ou le groupe social¬†?

1. **D√©finir des mesures d'√©quit√©**. Enfin, d√©finissez une m√©trique afin d'avoir quelque chose √† comparer dans votre travail pour am√©liorer la situation.

### Identifier les pr√©judices (et les avantages)

Quels sont les inconv√©nients et les avantages associ√©s au pr√™t ? Pensez aux faux n√©gatifs et aux faux positifs :

**Faux n√©gatifs** (rejeter, mais Y=1) - dans ce cas, un demandeur qui sera capable de rembourser un pr√™t est rejet√©. Il s'agit d'un √©v√©nement d√©favorable parce que les pr√™ts sont refus√©es aux candidats qualifi√©s.

**Faux positifs** (accepter, mais Y=0) - dans ce cas, le demandeur obtient un pr√™t mais finit par faire d√©faut. En cons√©quence, le dossier du demandeur sera envoy√© √† une agence de recouvrement de cr√©ances, ce qui peut affecter ses futures demandes de pr√™t.

### Identifier les groupes touch√©s

L'√©tape suivante consiste √† d√©terminer quels groupes sont susceptibles d'√™tre touch√©s. Par exemple, dans le cas d'une demande de carte de cr√©dit, un mod√®le pourrait d√©terminer que les femmes devraient recevoir des limites de cr√©dit beaucoup plus basses par rapport √† leurs conjoints qui partagent les biens du m√©nage. Tout un groupe d√©mographique, d√©fini par le sexe, est ainsi touch√©.

### D√©finir les mesures d'√©quit√©
 
Nous avons identifi√© les pr√©judices et un groupe affect√©, dans ce cas, d√©fini par leur sexe. Maintenant, nous pouvons utiliser les facteurs quantifi√©s pour d√©sagr√©ger leurs m√©triques. Par exemple, en utilisant les donn√©es ci-dessous, nous pouvons voir que les femmes ont le taux de faux positifs le plus √©lev√© et les hommes ont le plus petit, et que l'inverse est vrai pour les faux n√©gatifs.

‚úÖ Dans une prochaine le√ßon sur le clustering, nous verrons comment construire cette 'matrice de confusion' avec du code

|            | Taux de faux positifs | Taux de faux n√©gatifs | Nombre |
| ---------- | ------------------- | ------------------- | ----- |
| Femmes      | 0.37                | 0.27                | 54032 |
| Hommes        | 0.31                | 0.35                | 28620 |
| Non binaire | 0.33                | 0.31                | 1266  |

 
Ce tableau nous dit plusieurs choses. Premi√®rement, nous notons qu'il y a relativement peu de personnes non binaires dans les donn√©es. Les donn√©es sont fauss√©es, nous devons donc faire attention √† la fa√ßon dont nous allons interpr√©ter ces chiffres.

Dans ce cas, nous avons 3 groupes et 2 mesures. Lorsque nous pensons √† la mani√®re dont notre syst√®me affecte le groupe de clients avec leurs demandeurs de pr√™t, cela peut √™tre suffisant. Cependant si nous souhaitions d√©finir un plus grand nombre de groupes, nous allons s√ªrement devoir le r√©partir en de plus petits ensembles de mesures. Pour ce faire, vous pouvez ajouter plus de m√©triques, telles que la plus grande diff√©rence ou le plus petit rapport de chaque faux n√©gatif et faux positif.

‚úÖ Arr√™tez-vous et r√©fl√©chissez : Quels autres groupes sont susceptibles d'√™tre affect√©s par la demande de pr√™t ? 
 
## Att√©nuer l'injustice
 
Pour att√©nuer l'injustice, il faut explorer le mod√®le pour g√©n√©rer divers mod√®les att√©nu√©s et comparer les compromis qu'il fait entre pr√©cision et √©quit√© afin de s√©lectionner le mod√®le le plus √©quitable.

Cette le√ßon d'introduction ne plonge pas profond√©ment dans les d√©tails de l'att√©nuation des injustices algorithmiques, telles que l'approche du post-traitement et des r√©ductions, mais voici un outil que vous voudrez peut-√™tre essayer.

### Fairlearn 
 
[Fairlearn](https://fairlearn.github.io/) est un package Python open source qui permet d'√©valuer l'√©quit√© des syst√®mes et d'att√©nuer les injustices.

L'outil aide √† √©valuer comment les pr√©dictions d'un mod√®le affectent diff√©rents groupes, en permettant de comparer plusieurs mod√®les en utilisant des mesures d'√©quit√© et de performance, et en fournissant un ensemble d'algorithmes pour att√©nuer les injustices dans la classification binaire et la r√©gression.

- Apprenez √† utiliser les diff√©rents composants en consultant la documentation Fairlearn sur [GitHub](https://github.com/fairlearn/fairlearn/)

- Explorer le [guide utilisateur](https://fairlearn.github.io/main/user_guide/index.html), et les [exemples](https://fairlearn.github.io/main/auto_examples/index.html)

- Essayez quelques [notebooks d'exemples](https://github.com/fairlearn/fairlearn/tree/master/notebooks). 
  
- Apprenez [comment activer les √©valuations d'√©quit√©s](https://docs.microsoft.com/fr-fr/azure/machine-learning/how-to-machine-learning-fairness-aml?WT.mc_id=academic-15963-cxa) des mod√®les de machine learning sur Azure Machine Learning. 
  
- Jetez un coup d'oeil aux [notebooks d'exemples](https://github.com/Azure/MachineLearningNotebooks/tree/master/contrib/fairness) pour plus de sc√©narios d'√©valuation d'√©quit√©s sur Azure Machine Learning. 

---
## üöÄ Challenge 
 
Pour √©viter que des biais ne soient introduits en premier lieu, nous devrions¬†: 

- Avoir une diversit√© d'exp√©riences et de perspectives parmi les personnes travaillant sur les syst√®mes 
- Investir dans des ensembles de donn√©es qui refl√®tent la diversit√© de notre soci√©t√©
- D√©velopper de meilleures m√©thodes pour d√©tecter et corriger les biais lorsqu'ils surviennent

Pensez √† des sc√©narios de la vie r√©elle o√π l'injustice est √©vidente dans la construction et l'utilisation de mod√®les. Que devrions-nous consid√©rer d'autre ?

## [Quiz de validation des connaissances](https://white-water-09ec41f0f.azurestaticapps.net/quiz/6/?loc=fr)
## R√©vision et auto-apprentissage
 
Dans cette le√ßon, nous avons appris quelques notions de base sur les concepts d'√©quit√© et d'injustice dans le machine learning.  
 
Regardez cet atelier pour approfondir les sujets :

- YouTube : Dommages li√©s √† l'√©quit√© dans les syst√®mes d'IA¬†: exemples, √©valuation et att√©nuation par Hanna Wallach et Miro Dudik [Fairness-related harms in AI systems: Examples, assessment, and mitigation - YouTube](https://www.youtube.com/watch?v=1RptHwfkx_k) 

Lectures suppl√©mentaires : 

- Centre de ressources Microsoft RAI : [Responsible AI Resources ‚Äì Microsoft AI](https://www.microsoft.com/fr-fr/ai/responsible-ai-resources?activetab=pivot1:primaryr4&rtc=1) 

- Groupe de recherche Microsoft FATE : [FATE: Fairness, Accountability, Transparency, and Ethics in AI - Microsoft Research](https://www.microsoft.com/research/theme/fate/) 

Explorer la boite √† outils Fairlearn

[Fairlearn](https://fairlearn.org/)

Lire sur les outils Azure Machine Learning afin d'assurer l'√©quit√©

- [Azure Machine Learning](https://docs.microsoft.com/fr-fr/azure/machine-learning/concept-fairness-ml?WT.mc_id=academic-15963-cxa) 

## Devoir

[Explorer Fairlearn](assignment.fr.md) 
