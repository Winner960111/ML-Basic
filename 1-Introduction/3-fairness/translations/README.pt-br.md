# Equidade no Machine Learning

![Resumo de imparcialidade no Machine Learning em um sketchnote](../../../sketchnotes/ml-fairness.png)
> Sketchnote por [Tomomi Imura](https://www.twitter.com/girlie_mac)

## [Teste pr√©-aula](https://white-water-09ec41f0f.azurestaticapps.net/quiz/5?loc=ptbr)

## Introdu√ß√£o

Neste curso, voc√™ come√ßar√° a descobrir como o machine learning pode e est√° impactando nosso dia a dia. Mesmo agora, os sistemas e modelos est√£o envolvidos nas tarefas di√°rias de tomada de decis√£o, como diagn√≥sticos de sa√∫de ou detec√ß√£o de fraudes. Portanto, √© importante que esses modelos funcionem bem para fornecer resultados justos para todos.

Imagine o que pode acontecer quando os dados que voc√™ est√° usando para construir esses modelos n√£o t√™m certos dados demogr√°ficos, como ra√ßa, g√™nero, vis√£o pol√≠tica, religi√£o ou representam desproporcionalmente esses dados demogr√°ficos. E quando a sa√≠da do modelo √© interpretada para favorecer alguns dados demogr√°ficos? Qual √© a consequ√™ncia para a aplica√ß√£o?

Nesta li√ß√£o, voc√™ ir√°:

- Aumentar sua consci√™ncia sobre a import√¢ncia da imparcialidade no machine learning.
- Aprender sobre danos relacionados √† justi√ßa.
- Aprender sobre avalia√ß√£o e mitiga√ß√£o de injusti√ßas.

## Pr√©-requisito

Como pr√©-requisito, siga o Caminho de aprendizagem "Princ√≠pios de AI respons√°vel" e assista ao v√≠deo abaixo sobre o t√≥pico:

Saiba mais sobre a AI respons√°vel seguindo este [Caminho de aprendizagem](https://docs.microsoft.com/learn/modules/responsible-ai-principles/?WT.mc_id=academic-15963-cxa)

[![Abordagem da Microsoft para AI respons√°vel](https://img.youtube.com/vi/dnC8-uUZXSc/0.jpg)](https://youtu.be/dnC8-uUZXSc "Abordagem da Microsoft para AI respons√°vel")

> üé• Clique na imagem acima para ver um v√≠deo: Abordagem da Microsoft para AI respons√°vel

## Injusti√ßa em dados e algoritmos

> "Se voc√™ torturar os dados por tempo suficiente, eles confessar√£o qualquer coisa" - Ronald Coase

Essa afirma√ß√£o parece extrema, mas √© verdade que os dados podem ser manipulados para apoiar qualquer conclus√£o. Essa manipula√ß√£o √†s vezes pode acontecer de forma n√£o intencional. Como humanos, todos n√≥s temos preconceitos e muitas vezes √© dif√≠cil saber conscientemente quando voc√™ est√° introduzindo preconceitos nos dados.

Garantir a justi√ßa na AI e no machine learning continua sendo um desafio sociot√©cnico complexo. O que significa que n√£o pode ser abordado de perspectivas puramente sociais ou t√©cnicas.

### Danos relacionados √† justi√ßa

O que voc√™ quer dizer com injusti√ßa? ‚ÄúInjusti√ßa‚Äù abrange impactos negativos, ou ‚Äúdanos‚Äù, para um grupo de pessoas, tais como aqueles definidos em termos de ra√ßa, sexo, idade ou condi√ß√£o de defici√™ncia.  

Os principais danos relacionados √† justi√ßa podem ser classificados como:

- **Aloca√ß√£o**, se um g√™nero ou etnia, por exemplo, for favorecido em rela√ß√£o a outro.
- **Qualidade de servi√ßo**. Se voc√™ treinar os dados para um cen√°rio espec√≠fico, mas a realidade for muito mais complexa, isso levar√° a um servi√ßo de baixo desempenho.
- **Estereotipagem**. Associar um determinado grupo a atributos pr√©-atribu√≠dos.
- **Difama√ß√£o**. Criticar e rotular injustamente algo ou algu√©m..
- **Excesso ou falta de representa√ß√£o**. A ideia √© que determinado grupo n√£o seja visto em determinada profiss√£o, e qualquer servi√ßo ou fun√ß√£o que continue promovendo isso est√° contribuindo para o mal.

Vamos dar uma olhada nos exemplos.

### Aloca√ß√£o

Considere um sistema hipot√©tico para examinar os pedidos de empr√©stimo. O sistema tende a escolher homens brancos como melhores candidatos em rela√ß√£o a outros grupos. Como resultado, os empr√©stimos s√£o negados a certos candidatos.

Outro exemplo seria uma ferramenta de contrata√ß√£o experimental desenvolvida por uma grande empresa para selecionar candidatos. A ferramenta discriminou sistematicamente um g√™nero por meio dos modelos foram treinados para preferir palavras associadas a outro. Isso resultou na penaliza√ß√£o de candidatos cujos curr√≠culos continham palavras como "time feminino de r√∫gbi".

‚úÖ Fa√ßa uma pequena pesquisa para encontrar um exemplo do mundo real de algo assim

### Qualidade de servi√ßo

Os pesquisadores descobriram que v√°rios classificadores comerciais de g√™nero apresentavam taxas de erro mais altas em imagens de mulheres com tons de pele mais escuros, em oposi√ß√£o a imagens de homens com tons de pele mais claros. [Refer√™ncia](https://www.media.mit.edu/publications/gender-shades-intersectional-accuracy-disparities-in-commercial-gender-classification/)

Outro exemplo infame √© um distribuidor de sabonete para as m√£os que parecia n√£o ser capaz de detectar pessoas com pele escura. [Refer√™ncia](https://gizmodo.com/why-cant-this-soap-dispenser-identify-dark-skin-1797931773)

### Estereotipagem

Vis√£o de g√™nero estereot√≠pica foi encontrada na tradu√ß√£o autom√°tica. Ao traduzir ‚Äúele √© enfermeiro e ela m√©dica‚Äù para o turco, foram encontrados problemas. Turco √© uma l√≠ngua sem g√™nero que tem um pronome, ‚Äúo‚Äù para transmitir uma terceira pessoa do singular, mas traduzir a frase de volta do turco para o ingl√™s resulta no estere√≥tipo e incorreto como ‚Äúela √© uma enfermeira e ele √© um m√©dico‚Äù.

![translation to Turkish](../images/gender-bias-translate-en-tr.png)

![translation back to English](../images/gender-bias-translate-tr-en.png)

### Difama√ß√£o

Uma tecnologia de rotulagem de imagens erroneamente rotulou imagens de pessoas de pele escura como gorilas. Rotulagem incorreta √© prejudicial n√£o apenas porque o sistema cometeu um erro, pois aplicou especificamente um r√≥tulo que tem uma longa hist√≥ria de ser usado propositalmente para difamar os negros.

[![AI: N√£o sou uma mulher?](https://img.youtube.com/vi/QxuyfWoVV98/0.jpg)](https://www.youtube.com/watch?v=QxuyfWoVV98 "AI, N√£o sou uma mulher?")
> üé• Clique na imagem acima para ver o v√≠deo: AI, N√£o sou uma mulher - uma performance que mostra os danos causados pela difama√ß√£o racista da AI

### Excesso ou falta de representa√ß√£o

Resultados de pesquisa de imagens distorcidos podem ser um bom exemplo desse dano. Ao pesquisar imagens de profiss√µes com uma porcentagem igual ou maior de homens do que mulheres, como engenharia ou CEO, observe os resultados que s√£o mais inclinados para um determinado g√™nero.

![Pesquisa de CEO do Bing](../images/ceos.png)
> Esta pesquisa no Bing por 'CEO' produz resultados bastante inclusivos

Esses cinco tipos principais de danos n√£o s√£o mutuamente exclusivos e um √∫nico sistema pode exibir mais de um tipo de dano. Al√©m disso, cada caso varia em sua gravidade. Por exemplo, rotular injustamente algu√©m como criminoso √© um dano muito mais grave do que rotular erroneamente uma imagem. √â importante, no entanto, lembrar que mesmo danos relativamente n√£o graves podem fazer as pessoas se sentirem alienadas ou isoladas e o impacto cumulativo pode ser extremamente opressor.

‚úÖ **Discuss√£o**: Reveja alguns dos exemplos e veja se eles mostram danos diferentes.  

|                         |  Aloca√ß√£o  | Qualidade de servi√ßo | Estere√≥tipo | Difama√ß√£o | Excesso ou falta de representa√ß√£o |
| ----------------------- | :--------: | :----------------: | :----------: | :---------: | :----------------------------: |
| Sistema de contrata√ß√£o automatizado |     x      |         x          |      x       |             |               x                |
| Maquina de tradu√ß√£o     |            |                    |              |             |                                |
| Rotulagem de fotos          |            |                    |              |             |                                |

## Detectando injusti√ßa

Existem muitas raz√µes pelas quais um determinado sistema se comporta de maneira injusta. Vieses sociais, por exemplo, podem ser refletidos nos conjuntos de dados usados ‚Äã‚Äãpara trein√°-los. Por exemplo, a injusti√ßa na contrata√ß√£o pode ter sido exacerbada pela depend√™ncia excessiva de dados hist√≥ricos. Ao usar os padr√µes em curr√≠culos enviados √† empresa ao longo de um per√≠odo de 10 anos, o modelo determinou que os homens eram mais qualificados porque a maioria dos curr√≠culos vinha de homens, um reflexo do dom√≠nio masculino anterior na ind√∫stria de tecnologia.

Dados inadequados sobre um determinado grupo de pessoas podem ser motivo de injusti√ßa. Por exemplo, os classificadores de imagem t√™m maior taxa de erro para imagens de pessoas de pele escura porque os tons de pele mais escuros estavam subrepresentados nos dados.

Suposi√ß√µes erradas feitas durante o desenvolvimento tamb√©m causam injusti√ßa. Por exemplo, um sistema de an√°lise facial destinado a prever quem vai cometer um crime com base em imagens de rostos de pessoas pode levar a suposi√ß√µes prejudiciais. Isso pode levar a danos substanciais para as pessoas classificadas incorretamente.

## Entenda seus modelos e construa com justi√ßa

Embora muitos aspectos de justi√ßa n√£o sejam capturados em m√©tricas de justi√ßa quantitativas e n√£o seja poss√≠vel remover totalmente o preconceito de um sistema para garantir a justi√ßa, voc√™ ainda √© respons√°vel por detectar e mitigar os problemas de justi√ßa tanto quanto poss√≠vel.

Quando voc√™ est√° trabalhando com modelos de machine learning, √© importante entender seus modelos por meio de garantir sua interpretabilidade e avaliar e mitigar injusti√ßas.

Vamos usar o exemplo de sele√ß√£o de empr√©stimo para isolar o caso e descobrir o n√≠vel de impacto de cada fator na previs√£o.

## M√©todos de avalia√ß√£o

1. **Identifique os danos (e benef√≠cios)**. O primeiro passo √© identificar danos e benef√≠cios. Pense em como as a√ß√µes e decis√µes podem afetar os clientes em potencial e a pr√≥pria empresa.
  
2. **Identifique os grupos afetados**. Depois de entender que tipo de danos ou benef√≠cios podem ocorrer, identifique os grupos que podem ser afetados. Esses grupos s√£o definidos por g√™nero, etnia ou grupo social?

3. **Defina m√©tricas de justi√ßa**. Por fim, defina uma m√©trica para que voc√™ tenha algo para comparar em seu trabalho para melhorar a situa√ß√£o.

### Identificar danos (e benef√≠cios)

Quais s√£o os danos e benef√≠cios associados aos empr√©stimos? Pense em falsos negativos e cen√°rios de falsos positivos:

**Falsos negativos** (rejeitar, mas Y=1) - neste caso, um candidato que ser√° capaz de reembolsar um empr√©stimo √© rejeitado. Este √© um evento adverso porque os recursos dos empr√©stimos s√£o retidos de candidatos qualificados.

**Falsos positivos** ((aceitar, mas Y=0) - neste caso, o requerente obt√©m um empr√©stimo, mas acaba inadimplente. Como resultado, o caso do requerente ser√° enviado a uma ag√™ncia de cobran√ßa de d√≠vidas que pode afetar seus futuros pedidos de empr√©stimo.

### Identificar grupos afetados

A pr√≥xima etapa √© determinar quais grupos provavelmente ser√£o afetados. Por exemplo, no caso de um pedido de cart√£o de cr√©dito, um modelo pode determinar que as mulheres devem receber limites de cr√©dito muito mais baixos em compara√ß√£o com seus c√¥njuges que compartilham bens dom√©sticos. Todo um grupo demogr√°fico, definido por g√™nero, √© assim afetado.

### Definir m√©tricas de justi√ßa

Voc√™ identificou danos e um grupo afetado, neste caso, delineado por g√™nero. Agora, use os fatores quantificados para desagregar suas m√©tricas. Por exemplo, usando os dados abaixo, voc√™ pode ver que as mulheres t√™m a maior taxa de falsos positivos e os homens a menor, e que o oposto √© verdadeiro para falsos negativos.

‚úÖ Em uma li√ß√£o futura sobre Clustering, voc√™ ver√° como construir esta 'matriz de confus√£o' no c√≥digo

|            | Taxa de falsos positivos | Taxa de falsos negativos | contagem |
| ---------- | ------------------- | ------------------- | ----- |
| Mulheres   | 0.37                | 0.27                | 54032 |
| Homens     | 0.31                | 0.35                | 28620 |
| N√£o bin√°rio | 0.33                | 0.31                | 1266  |

Esta tabela nos diz v√°rias coisas. Primeiro, notamos que existem comparativamente poucas pessoas n√£o bin√°rias nos dados. Os dados est√£o distorcidos, ent√£o voc√™ precisa ter cuidado ao interpretar esses n√∫meros.

Nesse caso, temos 3 grupos e 2 m√©tricas. Quando estamos pensando em como nosso sistema afeta o grupo de clientes com seus solicitantes de empr√©stimos, isso pode ser suficiente, mas quando voc√™ deseja definir um n√∫mero maior de grupos, pode destilar isso em conjuntos menores de resumos. Para fazer isso, voc√™ pode adicionar mais m√©tricas, como a maior diferen√ßa ou menor propor√ß√£o de cada falso negativo e falso positivo.

‚úÖ Pare e pense: Que outros grupos provavelmente ser√£o afetados pelo pedido de empr√©stimo?

## Mitigando a injusti√ßa

Para mitigar a injusti√ßa, explore o modelo para gerar v√°rios modelos mitigados e compare as compensa√ß√µes que ele faz entre precis√£o e justi√ßa para selecionar o modelo mais justo.

Esta li√ß√£o introdut√≥ria n√£o se aprofunda nos detalhes da mitiga√ß√£o de injusti√ßa algor√≠tmica, como p√≥s-processamento e abordagem de redu√ß√µes, mas aqui est√° uma ferramenta que voc√™ pode querer experimentar.

### Fairlearn

[Fairlearn](https://fairlearn.github.io/) is an open-source Python package that allows you to assess your systems' fairness and mitigate unfairness.  

The tool helps you to assesses how a model's predictions affect different groups, enabling you to compare multiple models by using fairness and performance metrics, and supplying a set of algorithms to mitigate unfairness in binary classification and regression. 

- Learn how to use the different components by checking out the Fairlearn's [GitHub](https://github.com/fairlearn/fairlearn/)

- Explore the [user guide](https://fairlearn.github.io/main/user_guide/index.html), [examples](https://fairlearn.github.io/main/auto_examples/index.html)

- Try some [sample notebooks](https://github.com/fairlearn/fairlearn/tree/master/notebooks).
  
- Learn [how to enable fairness assessments](https://docs.microsoft.com/azure/machine-learning/how-to-machine-learning-fairness-aml?WT.mc_id=academic-15963-cxa) of machine learning models in Azure Machine Learning.
  
- Check out these [sample notebooks](https://github.com/Azure/MachineLearningNotebooks/tree/master/contrib/fairness) for more fairness assessment scenarios in Azure Machine Learning.

---
## üöÄ Desafio

Para evitar que preconceitos sejam introduzidos em primeiro lugar, devemos:

- t√™m uma diversidade de experi√™ncias e perspectivas entre as pessoas que trabalham em sistemas
- investir em conjuntos de dados que reflitam a diversidade de nossa sociedade
- desenvolver melhores m√©todos para detectar e corrigir preconceitos quando eles ocorrem

Pense em cen√°rios da vida real onde a injusti√ßa √© evidente na constru√ß√£o e uso de modelos. O que mais devemos considerar?

## [Question√°rio p√≥s-aula](https://white-water-09ec41f0f.azurestaticapps.net/quiz/6?loc=ptbr)

## Revis√£o e Autoestudo

Nesta li√ß√£o, voc√™ aprendeu alguns conceitos b√°sicos de justi√ßa e injusti√ßa no aprendizado de m√°quina.

Assista a este workshop para se aprofundar nos t√≥picos:

- YouTube: danos relacionados √† imparcialidade em sistemas de AI:  exemplos, avalia√ß√£o e mitiga√ß√£o por Hanna Wallach e Miro Dudik  [Danos relacionados √† imparcialidade em sistemas de AI: exemplos, avalia√ß√£o e mitiga√ß√£o - YouTube](https://www.youtube.com/watch?v=1RptHwfkx_k)

Al√©m disso, leia:

- Centro de recursos RAI da Microsoft: [Responsible AI Resources ‚Äì Microsoft AI](https://www.microsoft.com/ai/responsible-ai-resources?activetab=pivot1%3aprimaryr4)

- Grupo de pesquisa FATE da Microsoft: [FATE: Equidade, Responsabilidade, Transpar√™ncia e √âtica em IA - Microsoft Research](https://www.microsoft.com/research/theme/fate/)

Explore o kit de ferramentas Fairlearn

[Fairlearn](https://fairlearn.org/)

Leia sobre as ferramentas do Azure Machine Learning para garantir justi√ßa

- [Azure Machine Learning](https://docs.microsoft.com/azure/machine-learning/concept-fairness-ml?WT.mc_id=academic-15963-cxa)

## Tarefa

[Explore Fairlearn](assignment.pt-br.md)