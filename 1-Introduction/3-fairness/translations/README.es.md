# Justicia en el Aprendizaje Autom√°tico

![Resumen de justicia en el aprendizaje autom√°tico en un sketchnote](../../../sketchnotes/ml-fairness.png)
> Sketchnote por [Tomomi Imura](https://www.twitter.com/girlie_mac)

## [Examen previo a la lecci√≥n](https://white-water-09ec41f0f.azurestaticapps.net/quiz/5/)

## Introducci√≥n

En esta secci√≥n, comenzar√°s a descubrir como el aprendizaje autom√°tico puede y est√° impactando nuestra vida diaria. Incluso ahora mismo, hay sistemas y modelos involucrados en tareas diarias de toma de decisiones, como los diagn√≥sticos del cuidado de la salud o detecci√≥n del fraude. Es importante que estos modelos funcionen correctamente con el fin de proveer resultados justos para todos.

Imagina que podr√≠a pasar si los datos que usas para construir estos modelos carecen de cierta demograf√≠a, como es el caso de raza, g√©nero, punto de vista pol√≠tico, religi√≥n, o representan desproporcionadamente estas demograf√≠as. ¬øQu√© pasa cuando los resultados del modelo son interpretados en favor de alguna demograf√≠a? ¬øCu√°l es la consecuencia para la aplicaci√≥n?

En esta lecci√≥n, ser√° capaz de:

- Tomar conciencia de la importancia de la justicia en el aprendizaje autom√°tico.
- Aprender acerca de da√±os relacionados a la justicia.
- Aprender acerca de la evaluaci√≥n de la justicia y mitigaci√≥n.

## Prerrequisitos

Como un prerrequisito, por favor toma la ruta de aprendizaje "Responsible AI Principles" y mira el v√≠deo debajo sobre el tema:

Aprende m√°s acerca de la AI responsable siguiendo este [curso](https://docs.microsoft.com/es-es/learn/modules/responsible-ai-principles/?WT.mc_id=academic-15963-cxa)

[![Enfonque de Microsoft para la AI responsable](https://img.youtube.com/vi/dnC8-uUZXSc/0.jpg)](https://youtu.be/dnC8-uUZXSc "Enfoque de Microsoft para la AI responsable")

> üé• Haz clic en imagen superior para el v√≠deo: Enfoque de Microsoft para la AI responsable

## Injusticia en los datos y algoritmos

> "Si torturas los datos lo suficiente, estos confesar√°n cualquier cosa" - Ronald Coase

Esta oraci√≥n suena extrema, pero es cierto que los datos pueden ser manipulados para soportar cualquier conclusi√≥n. Dicha manipulaci√≥n puede ocurrir a veces de forma no intencional. Como humanos, todos tenemos sesgos, y muchas veces es dif√≠cil saber conscientemente cuando est√°s introduciendo un sesgo en los datos.

El garantizar la justicia en la AI y aprendizaje autom√°tico sigue siendo un desaf√≠o socio-tecnol√≥gico complejo. Esto quiere decir que no puede ser afrontado desde una perspectiva puramente social o t√©cnica.

### Da√±os relacionados con la justicia

¬øQu√© quieres decir con injusticia? "injusticia" engloba impactos negativos, o "da√±os", para un grupo de personas, como aquellos definidos en t√©rminos de raza, g√©nero, edad o estado de discapacidad.

Los principales da√±os relacionados a la justicia pueden ser clasificados como de:

- **Asignaci√≥n**, si un g√©nero o etnia, por ejemplo, se favorece sobre otro.
- **Calidad del servicio**. Si entrenas los datos para un escenario espec√≠fico pero la realidad es mucho m√°s compleja, esto conlleva a servicio de bajo rendimiento.
- **Estereotipo**. El asociar un cierto grupo con atributos preasignados.
- **Denigrado**. Criticar injustamente y etiquetar algo o alguien.
- **Sobre- o sub- representaci√≥n** La idea es que un cierto grupo no es visto en una cierta profesi√≥n, y cualquier servicio o funci√≥n que sigue promocion√°ndolo est√° contribuyendo al da√±o.

Demos un vistazo a los ejemplos.

### Asignaci√≥n

Considerar un sistema hipot√©tico para seleccionar solicitudes de pr√©stamo. El sistema tiende a seleccionar a hombres blancos como mejores candidatos por encima de otros grupos. Como resultado, los pr√©stamos se retienen para ciertos solicitantes.

Otro ejemplo ser√≠a una herramienta experimental de contrataci√≥n desarrollada por una gran corporaci√≥n para seleccionar candidatos. La herramienta discrimin√≥ sistem√°ticamente un g√©nero de otro usando los modelos entrenados para preferir palabras asociadas con otras, lo cual result√≥ en candidatos penalizados cuyos curr√≠culos contienen palabras como "equipo de rugby femenino".

‚úÖ Realiza una peque√±a investigaci√≥n para encontrar un ejemplo del mundo real de algo como esto.

### Calidad del servicio

Los investigadores encontraron que varios clasificadores de g√©nero comerciales ten√≠an altas tasas de error en las im√°genes de mujeres con tonos de piel m√°s oscuros, al contrario que con im√°genes de hombres con tonos de piel m√°s claros. [Referencia](https://www.media.mit.edu/publications/gender-shades-intersectional-accuracy-disparities-in-commercial-gender-classification/)

Otro ejemplo infame es el dispensador de jab√≥n para manos que parece no ser capaz de detectar a la gente con piel de color oscuro. [Referencia](https://gizmodo.com/why-cant-this-soap-dispenser-identify-dark-skin-1797931773)

### Estereotipo

La vista de g√©nero estereotipada fue encontrada en una traducci√≥n autom√°tica. Cuando se tradujo ‚Äú√âl es un enfermero y ella es una doctora‚Äù al turco, se encontraron los problemas. El turco es un idioma sin g√©nero el cual tiene un  pronombre "o" para comunicar el singular de la tercera persona, pero al traducir nuevamente la oraci√≥n del turco al ingl√©s resulta la frase estereotipada e incorrecta de ‚ÄúElla es una enfermera y √©l es un doctor‚Äù.

![Traducci√≥n al turco](../images/gender-bias-translate-en-tr.png)

![Traducci√≥n de nuevo al ingl√©s](../images/gender-bias-translate-tr-en.png)

### Denigraci√≥n

Una tecnolog√≠a de etiquetado de im√°genes horriblemente etiquet√≥ im√°genes de gente con color oscuro de piel como gorilas. El etiquetado incorrecto es da√±ino no solo porque el sistema cometi√≥ un error, sino porque espec√≠ficamente aplic√≥ una etiqueta que tiene una larga historia de ser usada a prop√≥sito para denigrar a la gente negra.

[![AI: ¬øNo soy una mujer?](https://img.youtube.com/vi/QxuyfWoVV98/0.jpg)](https://www.youtube.com/watch?v=QxuyfWoVV98 "AI, ¬øNo soy una mujer?")
> üé• Da clic en la imagen superior para el video: AI, ¬øNo soy una mujer? - un espect√°culo que muestra el da√±o causado por la denigraci√≥n racista de una AI.

### Sobre- o sub- representaci√≥n

Los resultados de b√∫squeda de im√°genes sesgados pueden ser un buen ejemplo de este da√±o. Cuando se buscan im√°genes de profesiones con un porcentaje igual o mayor de hombres que de mujeres, como en ingenier√≠a, o CEO, observa que los resultados est√°n mayormente inclinados hacia un g√©nero dado.

![B√∫squeda de CEO en Bing](../images/ceos.png)
> Esta b√∫squeda en Bing para 'CEO' produce resultados bastante inclusivos

Estos cinco tipos principales de da√±os no son mutuamente exclusivos, y un solo sistema puede exhibir m√°s de un tipo de da√±o. Adem√°s, cada caso var√≠a en severidad. Por ejemplo, etiquetar injustamente a alguien como un criminal es un da√±o mucho m√°s severo que etiquetar incorrectamente una imagen. Es importante, sin embargo, el recordar que a√∫n los da√±os relativamente no severos pueden hacer que la gente se sienta enajenada o se√±alada y el impacto acumulado puede ser extremadamente opresivo.
 
‚úÖ **Discusi√≥n**: Revisa algunos de los ejemplos y ve si estos muestran diferentes da√±os.

|                         | Asignaci√≥n | Calidad del servicio | Estereotipo | Denigrado | Sobre- o sub- representaci√≥n |
| ----------------------- | :--------: | :----------------: | :----------: | :---------: | :----------------------------: |
| Sistema de contrataci√≥n automatizada |     x      |         x          |      x       |             |               x                |
| Traducci√≥n autom√°tica   |            |                    |              |             |                                |
| Etiquetado de fotos        |            |                    |              |             |                                |


## Detectando injusticias

Hay varias razones por las que un sistema se comporta injustamente. Los sesgos sociales, por ejemplo, pueden ser reflejados en los conjutos de datos usados para entrenarlos. Por ejemplo, la injusticia en la contrataci√≥n puede ser exacerbada por la sobre dependencia en los datos hist√≥ricos. Al emplear patrones elaborados a partir de curr√≠culos enviados a la compa√±√≠a en un per√≠odo de 10 a√±os, el modelo determin√≥ que los hombres estaban m√°s calificados porque la mayor√≠a de los curr√≠culos proven√≠an de hombres, reflejo del pasado dominio masculino en la industria tecnol√≥gica.

Los datos inadecuados acerca de cierto grupo de personas pueden ser la raz√≥n de la injusticia. Por ejemplo, los clasificadores de im√°genes tienes una tasa de error m√°s alta para im√°genes de gente con piel oscura porque los tonos de piel m√°s oscura fueron sub-representados en los datos.

Las suposiciones err√≥neas hechas durante el desarrollo tambi√©n causan injusticia. Por ejemplo, un sistema de an√°lisis facial intent√≥ predecir qui√©n cometer√° un crimen basado en im√°genes de los rostros de personas que pueden llevar a supuestos da√±inos. Esto podr√≠a llevar a da√±os substanciales para las personas clasificadas err√≥neamente.

## Entiende tus modelos y construye de forma justa
 
A pesar de los muchos aspectos de justicia que no son capturados en m√©tricas cuantitativas justas, y que no es posible borrar totalmente el sesgo de un sistema para garantizar la justicia, eres responsable de detectar y mitigar problemas de justicia tanto como sea posible.

Cuando trabajas con modelos de aprendizaje autom√°tico, es importante entender tus modelos asegurando su interpretabilidad y evaluar y mitigar injusticias.

Usemos el ejemplo de selecci√≥n de pr√©stamos para aislar el caso y averiguar el nivel de impacto de cada factor en la predicci√≥n.

## M√©todos de evaluaci√≥n

1. **Identifica da√±os (y beneficios)**. El primer paso es identificar da√±os y beneficios. Piensa en c√≥mo las acciones y decisiones pueden afectar tanto a clientes potenciales como al negocio mismo.

2. **Identifica los grupos afectados**. Una vez que entendiste qu√© clase de da√±os o beneficios pueden ocurrir, identifica los grupos que podr√≠an ser afectados. ¬øEst√°n estos grupos definidos por g√©nero, etnicidad, o grupo social?

3. **Define m√©tricas de justicia**. Finalmente, define una m√©trica para as√≠ tener algo con qu√© medir en tu trabajo para mejorar la situaci√≥n.

### Identifica da√±os (y beneficios)

¬øCu√°les son los da√±os y beneficios asociados con el pr√©stamo? Piensa en escenarios con falsos negativos y falsos positivos: 

**Falsos negativos** (rechazado, pero Y=1) - en este caso, un solicitante que ser√≠a capaz de pagar un pr√©stamo es rechazado. Esto es un evento adverso porque los recursos de los pr√©stamos se retienen a los solicitantes calificados.

**Falsos positivos** (aceptado, pero Y=0) - en este caso, el solicitante obtiene un pr√©stamo pero eventualmente incumple. Como resultado, el caso del solicitante ser√° enviado a la agencia de cobro de deudas lo cual puede afectar en sus futuras solicitudes de pr√©stamo.

### Identifica los grupos afectados

Los siguientes pasos son determinar cuales son los grupos que suelen ser afectados. Por ejemplo, en caso de una solicitud de tarjeta de cr√©dito, un modelo puede determinar que las mujeres deber√≠an recibir mucho menor l√≠mite de cr√©dito comparado con sus esposos con los cuales comparten ingreso familiar. Una demograf√≠a entera, definida por g√©nero, es de este modo afectada.

### Define m√©tricas de justicia

Has identificado los da√±os y un grupo afectado, en este caso, delimitado por g√©nero. Ahora, usa los factores cuantificados para desagregar sus m√©tricas. Por ejemplo, usando los datos abajo, puedes ver que las mujeres tienen la tasa de falso positivo m√°s grande y los hombres tienen la m√°s peque√±a, y que lo opuesto es verdadero para los falsos negativos.

‚úÖ En una lecci√≥n futura de Clustering, ver√°s como construir esta 'matriz de confusi√≥n' en c√≥digo

|            | Tasa de falso positivo | Tasa de falso negativo | contador |
| ---------- | ------------------- | ------------------- | ----- |
| Mujeres    | 0.37                | 0.27                | 54032 |
| Hombres    | 0.31                | 0.35                | 28620 |
| No-binario | 0.33                | 0.31                | 1266  |

Esta tabla nos dice varias cosas. Primero, notamos que hay comparativamente pocas personas no-binarias en los datos. Los datos est√°n sesgados, por lo que necesitas ser cuidadoso en c√≥mo interpretas estos n√∫meros.

En este caso, tenemos 3 grupos y 2 m√©tricas. En el caso de c√≥mo nuestro sistema afecta a los grupos de clientes con sus solicitantes de pr√©stamo, esto puede ser suficiente, pero cuando quieres definir grupos mayores, querr√°s reducir esto a conjuntos m√°s peque√±os de res√∫menes. Para hacer eso, puedes agregar m√°s m√©tricas, como la mayor diferencia o la menor tasa de cada falso negativo y falso positivo.

‚úÖ Detente y piensa: ¬øQu√© otros grupos es probable se vean afectados a la hora de solicitar un pr√©stamo?

## Mitigando injusticias

Para mitigar injusticias, explora el modelo para generar varios modelos mitigados y compara las compensaciones que se hacen entre la precisi√≥n y justicia para seleccionar el modelo m√°s justo.

Esta lecci√≥n introductoria no profundiza en los detalles de mitigaci√≥n algor√≠tmica de injusticia, como los enfoques de post-procesado y de reducciones, pero aqu√≠ tienes una herramiento que podr√≠as probar:

### Fairlearn 

[Fairlearn](https://fairlearn.github.io/) es un paquete Python de c√≥digo abierto que te permite evaluar la justicia de tus sistemas y mitigar injusticias.

La herramienta te ayuda a evaluar c√≥mo unos modelos de predicci√≥n afectan a diferentes grupos, permiti√©ndote comparar m√∫ltiples modelos usando m√©tricas de rendimiento y justicia, y provee un conjunto de algoritmos para mitigar injusticia en regresi√≥n y clasificaci√≥n binaria.

- Aprende c√≥mo usar los distintos componentes revisando el repositorio de [GitHub](https://github.com/fairlearn/fairlearn/) de Fairlearn.

- Explora la [gu√≠a de usuario](https://fairlearn.github.io/main/user_guide/index.html), [ejemplos](https://fairlearn.github.io/main/auto_examples/index.html)

- Prueba algunos [notebooks de ejemplo](https://github.com/fairlearn/fairlearn/tree/master/notebooks).
  
- Aprende a [c√≥mo activar evaluaci√≥n de justicia](https://docs.microsoft.com/azure/machine-learning/how-to-machine-learning-fairness-aml?WT.mc_id=academic-15963-cxa) de los modelos de aprendizaje autom√°tico en Azure Machine Learning.
  
- Revisa estos [notebooks de ejemplo](https://github.com/Azure/MachineLearningNotebooks/tree/master/contrib/fairness) para m√°s escenarios de evaluaciones de justicia en Azure Machine Learning.

---
## üöÄ Desaf√≠o

Para prevenir que los sesgos sean introducidos en primer lugar, debemos:

- Tener una diversidad de antecedentes y perspectivas entre las personas trabajando en los sistemas.
- Invertir en conjuntos de datos que reflejen la diversidad de nuestra sociedad.
- Desarrollar mejores m√©todos para la detecci√≥n y correcci√≥n de sesgos cuando estos ocurren.

Piensa en escenarios de la vida real donde la injusticia es evidente en la construcci√≥n y uso de modelos. ¬øQu√© m√°s debemos considerar?

## [Cuestionario posterior a la lecci√≥n](https://white-water-09ec41f0f.azurestaticapps.net/quiz/6/)
## Revisi√≥n y autoestudio 

En esta lecci√≥n has aprendido algunos de los conceptos b√°sicos de justicia e injusticia en el aprendizaje autom√°tico.

Mira este taller para profundizar en estos temas:

- YouTube: [Da√±os relacionados con la justicia en sistemas de AI: Ejemplos, evaluaciones, y mitigaci√≥n - YouTube](https://www.youtube.com/watch?v=1RptHwfkx_k) por Hanna Wallach y Miro Dudik

Tambi√©n lee:

- Centro de recursos de Microsoft RAI: [Recursos de AI responsable ‚Äì Microsoft AI](https://www.microsoft.com/ai/responsible-ai-resources?activetab=pivot1%3aprimaryr4)

- Grupo de investigaci√≥n de Microsoft FATE: [FATE: Fairness, Accountability, Transparency, and Ethics in AI - Microsoft Research](https://www.microsoft.com/research/theme/fate/)

Explorar la caja de herramientas de Fairlearn

[Fairlearn](https://fairlearn.org/)

Lee acerca de las herramientas de Azure Machine Learning para asegurar justicia

- [Azure Machine Learning](https://docs.microsoft.com/azure/machine-learning/concept-fairness-ml?WT.mc_id=academic-15963-cxa)

## Tarea

[Explora Fairlearn](../translations/assignment.es.md)
